{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qdSlekJPeZ99"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # loss = model.weight_regularization_loss()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dQ3NBmWH5HXk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "class EvoDCNN(nn.Module):\n",
        "    def __init__(self, num_layers, num_filters, filter_size, activation_func, pooling_func, hidden_size, initialization, dropout_value, short_connection, batch_norm):\n",
        "        super(EvoDCNN, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        in_channels = 3 \n",
        "        self.short_connection= short_connection\n",
        "        \n",
        "        # define the layers of the CNN based on the hyperparameters\n",
        "        for i in range(num_layers):\n",
        "            layer = nn.Conv2d(in_channels, num_filters, filter_size)\n",
        "            initialization(layer.weight)\n",
        "            layers.append(layer)\n",
        "            if batch_norm:\n",
        "              layers.append(nn.BatchNorm2d(num_filters))\n",
        "            layers.append(activation_func())\n",
        "            layers.append(nn.Dropout(p=dropout_value))\n",
        "            # layers.append(pooling_func(filter_size))\n",
        "            in_channels = num_filters\n",
        "        \n",
        "        self.cnn_layers = nn.Sequential(*layers)\n",
        "        # print( 'num_layers:', num_layers,'num_filters',num_filters,'filter_size:',filter_size,'activation_func:',activation_func,'pooling_func:',pooling_func)\n",
        "        self.fc_layer = nn.Linear(hidden_size[0]*hidden_size[1]*hidden_size[2], 10) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x # save the input tensor for the short connection\n",
        "        x = self.cnn_layers(x)\n",
        "        # print('shape ',x.shape[1],x.shape[2],x.shape[3])\n",
        "        x = x.view(x.size(0), -1) # flatten the output of the CNN layers\n",
        "        # r = x.shape[1]#*x.shape[2]#*x.shape[3]\n",
        "        # x = nn.Linear(r, 10) \n",
        "        x = self.fc_layer(x)\n",
        "        if self.short_connection: # add the input tensor to the output of the convolutional block\n",
        "            identity = identity.view(identity.size(0), -1) # resize identity tensor\n",
        "            identity = identity[:, :x.shape[1]] # match the number of output features\n",
        "            x = x + identity\n",
        "        return x\n",
        "\n",
        "\n",
        "def calculate_size(num_layers, num_filters, filter_size, activation_func, pooling_func, input_size, dropout_value, batch_norm):\n",
        "    # Define a sequential model with the specified number of layers\n",
        "    model = nn.Sequential()\n",
        "    in_channels = input_size[0]\n",
        "    # print(in_channels,num_filters, filter_size)\n",
        "    for i in range(num_layers):\n",
        "        model.add_module(f'conv{i+1}', nn.Conv2d(in_channels, num_filters, filter_size))\n",
        "        model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_filters))\n",
        "        model.add_module(f'activation{i+1}', activation_func())\n",
        "        model.add_module(f'dropout{i+1}',nn.Dropout(p=dropout_value))\n",
        "        model.add_module(f'batch_norm{i+1}',nn.BatchNorm2d(num_filters))\n",
        "        # model.add_module(f'pooling{i+1}', pooling_func(filter_size))\n",
        "        in_channels = num_filters\n",
        "\n",
        "    # Calculate the output size of the model\n",
        "    x = torch.randn(input_size).unsqueeze(0)\n",
        "    output_size = model(x).size()[1:]\n",
        "    return output_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yPHFqhX3Dxd8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "hyperparameter_ranges = {\n",
        "    'num_layers': range(2, 5),\n",
        "    'num_filters': [8, 16, 32, 64, 128, 256],\n",
        "    'filter_size': [3, 5, 7],\n",
        "    'activation_func': [nn.ReLU, nn.Sigmoid],\n",
        "    'pooling_func': [nn.MaxPool2d, nn.AvgPool2d],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'optimizer': [optim.Adam, optim.SGD, optim.RMSprop, optim.Adagrad, optim.Adadelta],\n",
        "    'initialization': [nn.init.normal_, nn.init.uniform_, nn.init.trunc_normal_, \n",
        "                       nn.init.normal_, nn.init.xavier_normal_, nn.init.xavier_uniform_, \n",
        "                       nn.init.kaiming_normal_, nn.init.kaiming_uniform_],\n",
        "    'dropout': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    'short_connection': [0, 1],\n",
        "    'batch_norm': [0, 1]\n",
        "}\n",
        "\n",
        "# Define the function to generate a random population\n",
        "def generate_random_population(population_size, hyperparameter_ranges):\n",
        "    population = []\n",
        "    for i in range(population_size):\n",
        "        member = []\n",
        "        member.append(random.choice(hyperparameter_ranges['num_layers']))\n",
        "        member.append(random.choice(hyperparameter_ranges['num_filters']))\n",
        "        member.append(random.choice(hyperparameter_ranges['filter_size']))\n",
        "        member.append(random.choice(hyperparameter_ranges['activation_func']))   \n",
        "        member.append(random.choice(hyperparameter_ranges['pooling_func']))\n",
        "        member.append(random.choice(hyperparameter_ranges['learning_rate']))\n",
        "        member.append(random.choice(hyperparameter_ranges['optimizer']))\n",
        "        member.append(random.choice(hyperparameter_ranges['initialization']))\n",
        "        member.append(random.choice(hyperparameter_ranges['dropout']))\n",
        "        member.append(random.choice(hyperparameter_ranges['short_connection']))\n",
        "        member.append(random.choice(hyperparameter_ranges['batch_norm']))\n",
        "        population.append(member)\n",
        "    return population\n",
        "\n",
        "\n",
        "def evaluate_population(members, train_loader, test_loader, num_epochs=5):\n",
        "    best_member = None\n",
        "    best_loss = float('inf')\n",
        "    running_accuracy = 0.0\n",
        "    for i, member in enumerate(members):\n",
        "        input_size = (3,32,32)\n",
        "        y = calculate_size(member[0], member[1], member[2], member[3], member[4], input_size, member[8], member[10])\n",
        "        model = EvoDCNN(member[0], member[1], member[2], member[3], member[4], y, member[7], member[8], member[9], member[10])\n",
        "        #model = model.to(device)\n",
        "        if torch.cuda.is_available():\n",
        "            model.cuda()\n",
        "        model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = member[6](model.parameters(), lr=member[5])\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0     \n",
        "            for j, data in enumerate(train_loader, 0):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f\"Epoch {epoch+1}: loss {running_loss/len(train_loader)}\")\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "        # Add their accuracy\n",
        "        member.append(accuracy)\n",
        "        print(f\"Accuracy for member {i+1}: {accuracy}%\")\n",
        "        # print(running_accuracy,' < ',accuracy)\n",
        "        if running_accuracy < accuracy:\n",
        "            running_accuracy = accuracy\n",
        "            best_member = member\n",
        "            # print('current bm:',best_member)\n",
        "    print('Best member is:', best_member)\n",
        "    return best_member, members\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BEuqJ6MKDrf7"
      },
      "outputs": [],
      "source": [
        "# Define the mutation operator\n",
        "def mutation(member, hyperparameter_ranges):\n",
        "   mutated_member = member.copy()\n",
        "   param = ['num_layers', 'num_filters', 'filter_size', 'activation_func', 'pooling_func','learning_rate', 'optimizer', 'initialization', 'dropout', 'short_connection', 'batch_norm']\n",
        "   i = random.randint(0, 10)\n",
        "   new_param = random.choice(hyperparameter_ranges[param[i]])\n",
        "   while new_param == mutated_member[i]:\n",
        "      new_param = random.choice(hyperparameter_ranges[param[i]])\n",
        "   \n",
        "   mutated_member[i] = new_param\n",
        "\n",
        "   return mutated_member\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    crossover_point = np.random.randint(1, len(parent1))\n",
        "    child_chromosomes = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "    return child_chromosomes\n",
        "\n",
        "def select_parents(population, num_parents):\n",
        "    fitness_scores = [p[-1] for p in population]\n",
        "    total_fitness = sum(fitness_scores)\n",
        "    probabilities = [score/total_fitness for score in fitness_scores]\n",
        "    parents = []\n",
        "    for i in range(num_parents):\n",
        "        parent_idx = np.random.choice(len(population), p=probabilities)\n",
        "        parents.append(population[parent_idx])\n",
        "    return parents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sV-UuXP7DMeY"
      },
      "outputs": [],
      "source": [
        "# import torch.utils.data as data\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "def select_best_model(initial_train_data): \n",
        "    # Convert initial_train_data into a dataset object\n",
        "    train_dataset = initial_train_data.dataset\n",
        "\n",
        "    # Set the size of the test set\n",
        "    test_size = int(len(train_dataset) * 0.2) # 20% of the data will be used for testing\n",
        "\n",
        "    # Calculate the size of the training set\n",
        "    train_size = len(train_dataset) - test_size\n",
        "\n",
        "    # Use random_split to create a train and test dataset\n",
        "    train_dataset, test_dataset = data_utils.random_split(train_dataset, [train_size, test_size])\n",
        "\n",
        "    # Create a DataLoader for the train and test dataset\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
        "    \n",
        "    # Generate the initial population\n",
        "    population_size = 5\n",
        "    initial_population = generate_random_population(population_size, hyperparameter_ranges)\n",
        "    \n",
        "    # Evaluate the initial population\n",
        "    best_member, members = evaluate_population(initial_population, train_loader, test_loader)\n",
        "    \n",
        "    # Run the genetic algorithm\n",
        "    num_generations = 1\n",
        "    combined_population = members\n",
        "    for generation in range(num_generations):\n",
        "        print('Generation: ', generation)\n",
        "        # Select the parents for crossover\n",
        "        parents = select_parents(members, 2)\n",
        "        \n",
        "        # Perform crossover and mutation to create the new population\n",
        "        new_population = []\n",
        "        for i in range(population_size):\n",
        "            parent1 = parents[random.randint(0, len(parents)-1)]\n",
        "            parent2 = parents[random.randint(0, len(parents)-1)]\n",
        "            child = crossover(parent1, parent2)\n",
        "            child = mutation(child, hyperparameter_ranges)\n",
        "            new_population.append(child)\n",
        "        \n",
        "        # Evaluate the new population\n",
        "        new_best_member, new_members = evaluate_population(new_population, train_loader, test_loader)\n",
        "        combined_population = combined_population + new_members\n",
        "        members = new_members\n",
        "    sorted_list = sorted(combined_population, key=lambda x: x[-1])\n",
        "    final_best_member = sorted_list[-1]\n",
        "    print(\"Best member:\", final_best_member)\n",
        "    return final_best_member\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7xU1HMelJ3",
        "outputId": "59a70cf9-0590-4cc6-8161-84381e1fbc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 45091732.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Epoch 1: loss 2.467020674635427\n",
            "Epoch 2: loss 2.3552102311350667\n",
            "Epoch 3: loss 2.355137522609089\n",
            "Epoch 4: loss 2.35498519141834\n",
            "Epoch 5: loss 2.222622519103102\n",
            "Accuracy for member 1: 24.62%\n",
            "Epoch 1: loss 8.218504267378737\n",
            "Epoch 2: loss 2.358458701033181\n",
            "Epoch 3: loss 2.36049069535618\n",
            "Epoch 4: loss 2.3616379282345026\n",
            "Epoch 5: loss 2.360328037517901\n",
            "Accuracy for member 2: 10.25%\n",
            "Epoch 1: loss 1.9286543926872766\n",
            "Epoch 2: loss 1.6300531770474613\n",
            "Epoch 3: loss 1.4958377410047732\n",
            "Epoch 4: loss 1.4017783643338626\n",
            "Epoch 5: loss 1.31934947441942\n",
            "Accuracy for member 3: 53.4%\n",
            "Epoch 1: loss 2.9033129283795343\n",
            "Epoch 2: loss 2.3079451699607287\n",
            "Epoch 3: loss 2.3088459587706542\n",
            "Epoch 4: loss 2.309035978378198\n",
            "Epoch 5: loss 2.309220004005554\n",
            "Accuracy for member 4: 10.06%\n",
            "Epoch 1: loss 107740892.43127304\n",
            "Epoch 2: loss 2.692625802926743\n",
            "Epoch 3: loss 2.307408584954259\n",
            "Epoch 4: loss 2.3073035269118725\n",
            "Epoch 5: loss 2.3086846567952213\n",
            "Accuracy for member 5: 10.18%\n",
            "Best member is: [4, 64, 3, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.pooling.AvgPool2d'>, 0.01, <class 'torch.optim.sgd.SGD'>, <function xavier_normal_ at 0x7fb1040383a0>, 0.1, 0, 0, 53.4]\n",
            "Generation:  0\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "\n",
        "\n",
        "  best_model = select_best_model(train_data)\n",
        "  input_size = (3,32,32)\n",
        "  y = calculate_size(best_model[0], best_model[1], best_model[2], best_model[3], best_model[4], input_size, best_model[8], best_model[10])\n",
        "  model = EvoDCNN(best_model[0], best_model[1], best_model[2], best_model[3], best_model[4], y, best_model[7], best_model[8], best_model[9], best_model[10])\n",
        "  model.to(device)\n",
        "  optimizer = best_model[6](model.parameters(), lr=best_model[5])\n",
        "\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}