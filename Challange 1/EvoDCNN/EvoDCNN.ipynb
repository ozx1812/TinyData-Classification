{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV6DxGkPeQDD"
   },
   "source": [
    "***Challenge 1***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72hfgxjTd_lk"
   },
   "source": [
    "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk0Ilt_-duk2"
   },
   "source": [
    "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWyBTUe3idZI"
   },
   "source": [
    "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qdSlekJPeZ99"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N7soYNWEedl9"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        # loss = model.weight_regularization_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dQ3NBmWH5HXk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "class EvoDCNN(nn.Module):\n",
    "    def __init__(self, num_layers, num_filters, filter_size, activation_func, pooling_func, hidden_size, initialization, dropout_value, short_connection, batch_norm):\n",
    "        super(EvoDCNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 3 \n",
    "        self.short_connection= short_connection\n",
    "        \n",
    "        # define the layers of the CNN based on the hyperparameters\n",
    "        for i in range(num_layers):\n",
    "            layer = nn.Conv2d(in_channels, num_filters, filter_size)\n",
    "            initialization(layer.weight)\n",
    "            layers.append(layer)\n",
    "            if batch_norm:\n",
    "              layers.append(nn.BatchNorm2d(num_filters))\n",
    "            layers.append(activation_func())\n",
    "            layers.append(nn.Dropout(p=dropout_value))\n",
    "            # layers.append(pooling_func(filter_size))\n",
    "            in_channels = num_filters\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(*layers)\n",
    "        # print( 'num_layers:', num_layers,'num_filters',num_filters,'filter_size:',filter_size,'activation_func:',activation_func,'pooling_func:',pooling_func)\n",
    "        self.fc_layer = nn.Linear(hidden_size[0]*hidden_size[1]*hidden_size[2], 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x # save the input tensor for the short connection\n",
    "        x = self.cnn_layers(x)\n",
    "        # print('shape ',x.shape[1],x.shape[2],x.shape[3])\n",
    "        x = x.view(x.size(0), -1) # flatten the output of the CNN layers\n",
    "        # r = x.shape[1]#*x.shape[2]#*x.shape[3]\n",
    "        # x = nn.Linear(r, 10) \n",
    "        x = self.fc_layer(x)\n",
    "        if self.short_connection: # add the input tensor to the output of the convolutional block\n",
    "            identity = identity.view(identity.size(0), -1) # resize identity tensor\n",
    "            identity = identity[:, :x.shape[1]] # match the number of output features\n",
    "            x = x + identity\n",
    "        return x\n",
    "\n",
    "\n",
    "def calculate_size(num_layers, num_filters, filter_size, activation_func, pooling_func, input_size, dropout_value, batch_norm):\n",
    "    # Define a sequential model with the specified number of layers\n",
    "    model = nn.Sequential()\n",
    "    in_channels = input_size[0]\n",
    "    # print(in_channels,num_filters, filter_size)\n",
    "    for i in range(num_layers):\n",
    "        model.add_module(f'conv{i+1}', nn.Conv2d(in_channels, num_filters, filter_size))\n",
    "        model.add_module(f'batchnorm{i+1}', nn.BatchNorm2d(num_filters))\n",
    "        model.add_module(f'activation{i+1}', activation_func())\n",
    "        model.add_module(f'dropout{i+1}',nn.Dropout(p=dropout_value))\n",
    "        model.add_module(f'batch_norm{i+1}',nn.BatchNorm2d(num_filters))\n",
    "        # model.add_module(f'pooling{i+1}', pooling_func(filter_size))\n",
    "        in_channels = num_filters\n",
    "\n",
    "    # Calculate the output size of the model\n",
    "    x = torch.randn(input_size).unsqueeze(0)\n",
    "    output_size = model(x).size()[1:]\n",
    "    return output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yPHFqhX3Dxd8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "hyperparameter_ranges = {\n",
    "    'num_layers': range(2, 5),\n",
    "    'num_filters': [8, 16, 32, 64, 128, 256],\n",
    "    'filter_size': [3, 5, 7],\n",
    "    'activation_func': [nn.ReLU, nn.Sigmoid],\n",
    "    'pooling_func': [nn.MaxPool2d, nn.AvgPool2d],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'optimizer': [optim.Adam, optim.SGD, optim.RMSprop, optim.Adagrad, optim.Adadelta],\n",
    "    'initialization': [nn.init.normal_, nn.init.uniform_, nn.init.trunc_normal_, \n",
    "                       nn.init.normal_, nn.init.xavier_normal_, nn.init.xavier_uniform_, \n",
    "                       nn.init.kaiming_normal_, nn.init.kaiming_uniform_],\n",
    "    'dropout': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    'short_connection': [0, 1],\n",
    "    'batch_norm': [0, 1]\n",
    "}\n",
    "\n",
    "# Define the function to generate a random population\n",
    "def generate_random_population(population_size, hyperparameter_ranges):\n",
    "    population = []\n",
    "    for i in range(population_size):\n",
    "        member = []\n",
    "        member.append(random.choice(hyperparameter_ranges['num_layers']))\n",
    "        member.append(random.choice(hyperparameter_ranges['num_filters']))\n",
    "        member.append(random.choice(hyperparameter_ranges['filter_size']))\n",
    "        member.append(random.choice(hyperparameter_ranges['activation_func']))   \n",
    "        member.append(random.choice(hyperparameter_ranges['pooling_func']))\n",
    "        member.append(random.choice(hyperparameter_ranges['learning_rate']))\n",
    "        member.append(random.choice(hyperparameter_ranges['optimizer']))\n",
    "        member.append(random.choice(hyperparameter_ranges['initialization']))\n",
    "        member.append(random.choice(hyperparameter_ranges['dropout']))\n",
    "        member.append(random.choice(hyperparameter_ranges['short_connection']))\n",
    "        member.append(random.choice(hyperparameter_ranges['batch_norm']))\n",
    "        population.append(member)\n",
    "    return population\n",
    "\n",
    "\n",
    "def evaluate_population(members, train_loader, test_loader, num_epochs=5):\n",
    "    best_member = None\n",
    "    best_loss = float('inf')\n",
    "    running_accuracy = 0.0\n",
    "    for i, member in enumerate(members):\n",
    "        input_size = (3,32,32)\n",
    "        y = calculate_size(member[0], member[1], member[2], member[3], member[4], input_size, member[8], member[10])\n",
    "        model = EvoDCNN(member[0], member[1], member[2], member[3], member[4], y, member[7], member[8], member[9], member[10])\n",
    "        #model = model.to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = member[6](model.parameters(), lr=member[5])\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0     \n",
    "            for j, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}: loss {running_loss/len(train_loader)}\")\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        # Add their accuracy\n",
    "        member.append(accuracy)\n",
    "        print(f\"Accuracy for member {i+1}: {accuracy}%\")\n",
    "        # print(running_accuracy,' < ',accuracy)\n",
    "        if running_accuracy < accuracy:\n",
    "            running_accuracy = accuracy\n",
    "            best_member = member\n",
    "            # print('current bm:',best_member)\n",
    "    print('Best member is:', best_member)\n",
    "    return best_member, members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BEuqJ6MKDrf7"
   },
   "outputs": [],
   "source": [
    "# Define the mutation operator\n",
    "def mutation(member, hyperparameter_ranges):\n",
    "   mutated_member = member.copy()\n",
    "   param = ['num_layers', 'num_filters', 'filter_size', 'activation_func', 'pooling_func','learning_rate', 'optimizer', 'initialization', 'dropout', 'short_connection', 'batch_norm']\n",
    "   i = random.randint(0, 10)\n",
    "   new_param = random.choice(hyperparameter_ranges[param[i]])\n",
    "   while new_param == mutated_member[i]:\n",
    "      new_param = random.choice(hyperparameter_ranges[param[i]])\n",
    "   \n",
    "   mutated_member[i] = new_param\n",
    "\n",
    "   return mutated_member\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    crossover_point = np.random.randint(1, len(parent1))\n",
    "    child_chromosomes = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    return child_chromosomes\n",
    "\n",
    "def select_parents(population, num_parents):\n",
    "    fitness_scores = [p[-1] for p in population]\n",
    "    total_fitness = sum(fitness_scores)\n",
    "    probabilities = [score/total_fitness for score in fitness_scores]\n",
    "    parents = []\n",
    "    for i in range(num_parents):\n",
    "        parent_idx = np.random.choice(len(population), p=probabilities)\n",
    "        parents.append(population[parent_idx])\n",
    "    return parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sV-UuXP7DMeY"
   },
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "def select_best_model(initial_train_data): \n",
    "    # Convert initial_train_data into a dataset object\n",
    "    train_dataset = initial_train_data.dataset\n",
    "\n",
    "    # Set the size of the test set\n",
    "    test_size = int(len(train_dataset) * 0.2) # 20% of the data will be used for testing\n",
    "\n",
    "    # Calculate the size of the training set\n",
    "    train_size = len(train_dataset) - test_size\n",
    "\n",
    "    # Use random_split to create a train and test dataset\n",
    "    train_dataset, test_dataset = data_utils.random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "    # Create a DataLoader for the train and test dataset\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Generate the initial population\n",
    "    population_size = 20\n",
    "    initial_population = generate_random_population(population_size, hyperparameter_ranges)\n",
    "    \n",
    "    # Evaluate the initial population\n",
    "    best_member, members = evaluate_population(initial_population, train_loader, test_loader)\n",
    "    \n",
    "    # Run the genetic algorithm\n",
    "    num_generations = 1\n",
    "    combined_population = members\n",
    "    for generation in range(num_generations):\n",
    "        print('Generation: ', generation)\n",
    "        # Select the parents for crossover\n",
    "        parents = select_parents(members, 2)\n",
    "        \n",
    "        # Perform crossover and mutation to create the new population\n",
    "        new_population = []\n",
    "        for i in range(population_size):\n",
    "            parent1 = parents[random.randint(0, len(parents)-1)]\n",
    "            parent2 = parents[random.randint(0, len(parents)-1)]\n",
    "            child = crossover(parent1, parent2)\n",
    "            child = mutation(child, hyperparameter_ranges)\n",
    "            new_population.append(child)\n",
    "        \n",
    "        # Evaluate the new population\n",
    "        new_best_member, new_members = evaluate_population(new_population, train_loader, test_loader)\n",
    "        combined_population = combined_population + new_members\n",
    "        members = new_members\n",
    "    sorted_list = sorted(combined_population, key=lambda x: x[-1])\n",
    "    final_best_member = sorted_list[-1]\n",
    "    print(\"Best member:\", final_best_member)\n",
    "    return final_best_member\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v7xU1HMelJ3",
    "outputId": "59a70cf9-0590-4cc6-8161-84381e1fbc56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afandi/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/afandi/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Epoch 1: loss 1.8179325524229593\n",
      "Epoch 2: loss 1.406090338009234\n",
      "Epoch 3: loss 1.2475179310043019\n",
      "Epoch 4: loss 1.128984160316638\n",
      "Epoch 5: loss 1.0304530299128816\n",
      "Accuracy for member 1: 57.88%\n",
      "Epoch 1: loss 2.290213839314616\n",
      "Epoch 2: loss 2.1216411072606096\n",
      "Epoch 3: loss 2.069930291785219\n",
      "Epoch 4: loss 2.0370296784483206\n",
      "Epoch 5: loss 2.00777700419624\n",
      "Accuracy for member 2: 29.11%\n",
      "Epoch 1: loss 305.741306028427\n",
      "Epoch 2: loss 2.3035023044854306\n",
      "Epoch 3: loss 2.303188974484087\n",
      "Epoch 4: loss 2.3027769414761576\n",
      "Epoch 5: loss 2.3029855851548167\n",
      "Accuracy for member 3: 9.86%\n",
      "Epoch 1: loss 3365.7781206319887\n",
      "Epoch 2: loss 2975.594572902107\n",
      "Epoch 3: loss 2819.0811592077675\n",
      "Epoch 4: loss 2685.3257545739316\n",
      "Epoch 5: loss 2587.9805467501997\n",
      "Accuracy for member 4: 30.02%\n",
      "Epoch 1: loss 4.276321762286055\n",
      "Epoch 2: loss 2.2827557134932985\n",
      "Epoch 3: loss 1.9092674710499211\n",
      "Epoch 4: loss 1.7128849160937836\n",
      "Epoch 5: loss 1.4803029525394258\n",
      "Accuracy for member 5: 52.86%\n",
      "Epoch 1: loss 2.3313032803824916\n",
      "Epoch 2: loss 2.2615957564820115\n",
      "Epoch 3: loss 2.059036194706877\n",
      "Epoch 4: loss 2.0329384426720227\n",
      "Epoch 5: loss 2.0221833104904468\n",
      "Accuracy for member 6: 25.88%\n",
      "Epoch 1: loss 2.1602718430205274\n",
      "Epoch 2: loss 1.9234645008659972\n",
      "Epoch 3: loss 1.8535313453918068\n",
      "Epoch 4: loss 1.8095027996709172\n",
      "Epoch 5: loss 1.777686934501599\n",
      "Accuracy for member 7: 36.87%\n",
      "Epoch 1: loss 2.3736459298636587\n",
      "Epoch 2: loss 1.4042131226664534\n",
      "Epoch 3: loss 1.1945937045466024\n",
      "Epoch 4: loss 1.0814962131908525\n",
      "Epoch 5: loss 0.9875088808254693\n",
      "Accuracy for member 8: 61.59%\n",
      "Epoch 1: loss 29.127772517097643\n",
      "Epoch 2: loss 27.6877389852993\n",
      "Epoch 3: loss 26.62026551584847\n",
      "Epoch 4: loss 25.866028989846715\n",
      "Epoch 5: loss 25.22887753983275\n",
      "Accuracy for member 9: 19.46%\n",
      "Epoch 1: loss 17.208882874574144\n",
      "Epoch 2: loss 1.6000619627797186\n",
      "Epoch 3: loss 1.4741896642282748\n",
      "Epoch 4: loss 1.3525430550590491\n",
      "Epoch 5: loss 1.2944129840634502\n",
      "Accuracy for member 10: 51.23%\n",
      "Epoch 1: loss 2.4984969041598872\n",
      "Epoch 2: loss 2.470252088083627\n",
      "Epoch 3: loss 2.4388745181476725\n",
      "Epoch 4: loss 2.4230177836677136\n",
      "Epoch 5: loss 2.3941254836682693\n",
      "Accuracy for member 11: 12.27%\n",
      "Epoch 1: loss 2.232867722313244\n",
      "Epoch 2: loss 2.120569480874668\n",
      "Epoch 3: loss 2.0829908059427913\n",
      "Epoch 4: loss 2.031228972319216\n",
      "Epoch 5: loss 1.9927575652972578\n",
      "Accuracy for member 12: 28.15%\n",
      "Epoch 1: loss 424514.385472792\n",
      "Epoch 2: loss 4.0720929749095784\n",
      "Epoch 3: loss 2.643879576612966\n",
      "Epoch 4: loss 2.3753930783500308\n",
      "Epoch 5: loss 2.32132353569372\n",
      "Accuracy for member 13: 9.82%\n",
      "Epoch 1: loss 692021626.1684192\n",
      "Epoch 2: loss 2.3600798506325424\n",
      "Epoch 3: loss 12.251458355413078\n",
      "Epoch 4: loss 2.3604026305408903\n",
      "Epoch 5: loss 2.934319430646805\n",
      "Accuracy for member 14: 11.68%\n",
      "Epoch 1: loss 32653.682269020963\n",
      "Epoch 2: loss 2.6965227713600135\n",
      "Epoch 3: loss 2.3315562120260904\n",
      "Epoch 4: loss 2.331352348144824\n",
      "Epoch 5: loss 2.327513192789242\n",
      "Accuracy for member 15: 10.03%\n",
      "Epoch 1: loss 2.224981677417938\n",
      "Epoch 2: loss 2.003155262706379\n",
      "Epoch 3: loss 1.8990600604218797\n",
      "Epoch 4: loss 1.8305866143193108\n",
      "Epoch 5: loss 1.7791175773730292\n",
      "Accuracy for member 16: 37.82%\n",
      "Epoch 1: loss 2.372569298210997\n",
      "Epoch 2: loss 2.181729078673707\n",
      "Epoch 3: loss 2.1029625129395018\n",
      "Epoch 4: loss 2.0565342175693937\n",
      "Epoch 5: loss 2.018461494780958\n",
      "Accuracy for member 17: 30.46%\n",
      "Epoch 1: loss 1472673722119.361\n",
      "Epoch 2: loss 59638074482.50479\n",
      "Epoch 3: loss 31986744516.29393\n",
      "Epoch 4: loss 19722567801.047924\n",
      "Epoch 5: loss 13258282803.527157\n",
      "Accuracy for member 18: 13.98%\n",
      "Epoch 1: loss 2.0279254102097535\n",
      "Epoch 2: loss 1.8212300858939419\n",
      "Epoch 3: loss 1.6640040428874592\n",
      "Epoch 4: loss 1.5851862103032608\n",
      "Epoch 5: loss 1.495055353679596\n",
      "Accuracy for member 19: 48.06%\n",
      "Epoch 1: loss 2.3112651913310773\n",
      "Epoch 2: loss 2.261182477299017\n",
      "Epoch 3: loss 2.221745818567733\n",
      "Epoch 4: loss 2.1878070336180375\n",
      "Epoch 5: loss 2.159027261094163\n",
      "Accuracy for member 20: 21.09%\n",
      "Best member is: [2, 64, 7, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.pooling.AvgPool2d'>, 0.01, <class 'torch.optim.sgd.SGD'>, <function xavier_uniform_ at 0x7f95f2167a60>, 0.2, 0, 1, 61.59]\n",
      "Generation:  0\n",
      "Epoch 1: loss 2.7363718759518463\n",
      "Epoch 2: loss 1.3524361498439654\n",
      "Epoch 3: loss 1.1235046312450983\n",
      "Epoch 4: loss 1.0059477456461507\n",
      "Epoch 5: loss 0.9254327585902838\n",
      "Accuracy for member 1: 59.75%\n",
      "Epoch 1: loss 2.3197040260790254\n",
      "Epoch 2: loss 2.302695651404774\n",
      "Epoch 3: loss 2.3026909858654863\n",
      "Epoch 4: loss 2.302676716551613\n",
      "Epoch 5: loss 2.3026943039208554\n",
      "Accuracy for member 2: 9.86%\n",
      "Epoch 1: loss 2.0126383213189465\n",
      "Epoch 2: loss 1.8434468378274205\n",
      "Epoch 3: loss 1.7535962182492875\n",
      "Epoch 4: loss 1.7035859286213835\n",
      "Epoch 5: loss 1.6607121263449185\n",
      "Accuracy for member 3: 37.91%\n",
      "Epoch 1: loss 6.997525799388702\n",
      "Epoch 2: loss 1.4895425973989713\n",
      "Epoch 3: loss 1.343145016283273\n",
      "Epoch 4: loss 1.2310156111900037\n",
      "Epoch 5: loss 1.132373484178854\n",
      "Accuracy for member 4: 57.95%\n",
      "Epoch 1: loss 1.7294215946532667\n",
      "Epoch 2: loss 1.380900392136254\n",
      "Epoch 3: loss 1.2293951178130251\n",
      "Epoch 4: loss 1.1133949181523186\n",
      "Epoch 5: loss 1.0308898601669092\n",
      "Accuracy for member 5: 60.03%\n",
      "Epoch 1: loss 1.9940655894172838\n",
      "Epoch 2: loss 1.6680058667454094\n",
      "Epoch 3: loss 1.5182494401170041\n",
      "Epoch 4: loss 1.421587233726209\n",
      "Epoch 5: loss 1.3419497226373838\n",
      "Accuracy for member 6: 47.61%\n",
      "Epoch 1: loss 2.2224899743692563\n",
      "Epoch 2: loss 1.4979931896867844\n",
      "Epoch 3: loss 1.3216168968060527\n",
      "Epoch 4: loss 1.1900078882805456\n",
      "Epoch 5: loss 1.0935790291228615\n",
      "Accuracy for member 7: 54.94%\n",
      "Epoch 1: loss 1.7028688020980396\n",
      "Epoch 2: loss 1.3974051833533632\n",
      "Epoch 3: loss 1.2614441074121494\n",
      "Epoch 4: loss 1.162087408308023\n",
      "Epoch 5: loss 1.0923094524743078\n",
      "Accuracy for member 8: 56.65%\n",
      "Epoch 1: loss 2.3008262539823976\n",
      "Epoch 2: loss 1.4135547449794441\n",
      "Epoch 3: loss 1.2223464178201109\n",
      "Epoch 4: loss 1.0840246144194192\n",
      "Epoch 5: loss 1.0124898400550453\n",
      "Accuracy for member 9: 62.73%\n",
      "Epoch 1: loss 2.312217351346732\n",
      "Epoch 2: loss 1.4724509636053262\n",
      "Epoch 3: loss 1.231182875534216\n",
      "Epoch 4: loss 1.104502593556913\n",
      "Epoch 5: loss 1.0081350203520192\n",
      "Accuracy for member 10: 60.67%\n",
      "Epoch 1: loss 1.5921458828563508\n",
      "Epoch 2: loss 1.2398673474979096\n",
      "Epoch 3: loss 1.0585049756418783\n",
      "Epoch 4: loss 0.9217935383510285\n",
      "Epoch 5: loss 0.8109666491849735\n",
      "Accuracy for member 11: 60.55%\n",
      "Epoch 1: loss 6.406729795300541\n",
      "Epoch 2: loss 1.447139723232379\n",
      "Epoch 3: loss 1.3138281895329778\n",
      "Epoch 4: loss 1.2016376586386952\n",
      "Epoch 5: loss 1.1042424086183786\n",
      "Accuracy for member 12: 58.47%\n",
      "Epoch 1: loss 1.5938082838210816\n",
      "Epoch 2: loss 1.243330084858611\n",
      "Epoch 3: loss 1.1410517536413174\n",
      "Epoch 4: loss 1.0729371071242677\n",
      "Epoch 5: loss 1.0258315357918175\n",
      "Accuracy for member 13: 61.84%\n",
      "Epoch 1: loss 2.1168279369799095\n",
      "Epoch 2: loss 1.9569003772430908\n",
      "Epoch 3: loss 1.8967453816447395\n",
      "Epoch 4: loss 1.8641945535001663\n",
      "Epoch 5: loss 1.842134254047284\n",
      "Accuracy for member 14: 35.53%\n",
      "Epoch 1: loss 2.0905373542072674\n",
      "Epoch 2: loss 1.8793074962810967\n",
      "Epoch 3: loss 1.7441282318042108\n",
      "Epoch 4: loss 1.6515822254430752\n",
      "Epoch 5: loss 1.5646135536626504\n",
      "Accuracy for member 15: 44.78%\n",
      "Epoch 1: loss 2.1823886191121304\n",
      "Epoch 2: loss 1.9689691112445185\n",
      "Epoch 3: loss 1.8763580573633456\n",
      "Epoch 4: loss 1.8069688279788716\n",
      "Epoch 5: loss 1.7579404012844586\n",
      "Accuracy for member 16: 39.51%\n",
      "Epoch 1: loss 1.6530551384813108\n",
      "Epoch 2: loss 1.2924036918737638\n",
      "Epoch 3: loss 1.1152129731239222\n",
      "Epoch 4: loss 0.9908163591314809\n",
      "Epoch 5: loss 0.8778372203199246\n",
      "Accuracy for member 17: 65.55%\n",
      "Epoch 1: loss 1.6420340412340986\n",
      "Epoch 2: loss 1.2790589298303134\n",
      "Epoch 3: loss 1.1056217435068978\n",
      "Epoch 4: loss 0.9801988426488809\n",
      "Epoch 5: loss 0.8771531197209709\n",
      "Accuracy for member 18: 66.89%\n",
      "Epoch 1: loss 2.0263016330548367\n",
      "Epoch 2: loss 1.4438943432542843\n",
      "Epoch 3: loss 1.2699076272428227\n",
      "Epoch 4: loss 1.1600236290940842\n",
      "Epoch 5: loss 1.0727792088977826\n",
      "Accuracy for member 19: 59.79%\n",
      "Epoch 1: loss 5.27899928405262\n",
      "Epoch 2: loss 3.583879730191094\n",
      "Epoch 3: loss 2.50773766703499\n",
      "Epoch 4: loss 2.253553675767332\n",
      "Epoch 5: loss 2.0788020009811694\n",
      "Accuracy for member 20: 41.2%\n",
      "Best member is: [4, 128, 7, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.pooling.AvgPool2d'>, 0.001, <class 'torch.optim.adam.Adam'>, <function xavier_normal_ at 0x7f95f2167af0>, 0.2, 0, 1, 48.06, 66.89]\n",
      "Best member: [4, 128, 7, <class 'torch.nn.modules.activation.ReLU'>, <class 'torch.nn.modules.pooling.AvgPool2d'>, 0.001, <class 'torch.optim.adam.Adam'>, <function xavier_normal_ at 0x7f95f2167af0>, 0.2, 0, 1, 48.06, 66.89]\n",
      "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.391518\n",
      "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.080782\n",
      "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.006429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [0/50 (0%)]\tLoss: 0.001383\n",
      "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.000432\n",
      "Train Epoch: 25 [0/50 (0%)]\tLoss: 0.000266\n",
      "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.000173\n",
      "Train Epoch: 35 [0/50 (0%)]\tLoss: 0.000159\n",
      "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.000178\n",
      "Train Epoch: 45 [0/50 (0%)]\tLoss: 0.000068\n",
      "Train Epoch: 50 [0/50 (0%)]\tLoss: 0.000059\n",
      "Train Epoch: 55 [0/50 (0%)]\tLoss: 0.000038\n",
      "Train Epoch: 60 [0/50 (0%)]\tLoss: 0.000031\n",
      "Train Epoch: 65 [0/50 (0%)]\tLoss: 0.000035\n",
      "Train Epoch: 70 [0/50 (0%)]\tLoss: 0.000039\n",
      "Train Epoch: 75 [0/50 (0%)]\tLoss: 0.000031\n",
      "Train Epoch: 80 [0/50 (0%)]\tLoss: 0.000029\n",
      "Train Epoch: 85 [0/50 (0%)]\tLoss: 0.000032\n",
      "Train Epoch: 90 [0/50 (0%)]\tLoss: 0.000023\n",
      "Train Epoch: 95 [0/50 (0%)]\tLoss: 0.000028\n",
      "\n",
      "Test set: Average loss: 1.7212, Accuracy: 266/400 (66.50%)\n",
      "\n",
      "Num Samples For Training 50 Num Samples For Val 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afandi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 12.843808093390907\n",
      "Epoch 2: loss 9.990034980895802\n",
      "Epoch 3: loss 9.512270478013987\n",
      "Epoch 4: loss 9.227900106685992\n",
      "Epoch 5: loss 8.96996532842374\n",
      "Accuracy for member 1: 23.5%\n",
      "Epoch 1: loss 218.23278593407653\n",
      "Epoch 2: loss 38.82736699649701\n",
      "Epoch 3: loss 27.445606125048556\n",
      "Epoch 4: loss 25.210592425288485\n",
      "Epoch 5: loss 20.338693670571423\n",
      "Accuracy for member 2: 25.01%\n",
      "Epoch 1: loss 124.89215384199977\n",
      "Epoch 2: loss 16.79520242587446\n",
      "Epoch 3: loss 8.822486243689784\n",
      "Epoch 4: loss 4.38533918545269\n",
      "Epoch 5: loss 2.700063821987603\n",
      "Accuracy for member 3: 36.94%\n",
      "Epoch 1: loss 1527.8401586758062\n",
      "Epoch 2: loss 836.8325744238905\n",
      "Epoch 3: loss 520.1550268593687\n",
      "Epoch 4: loss 349.5166550413869\n",
      "Epoch 5: loss 240.60933439114604\n",
      "Accuracy for member 4: 25.31%\n",
      "Epoch 1: loss 2.1855724558662684\n",
      "Epoch 2: loss 2.158107530575591\n",
      "Epoch 3: loss 2.1512711322345672\n",
      "Epoch 4: loss 2.146540874871202\n",
      "Epoch 5: loss 2.1449955324776258\n",
      "Accuracy for member 5: 23.78%\n",
      "Epoch 1: loss 7.676189622178245\n",
      "Epoch 2: loss 2.3031484052396047\n",
      "Epoch 3: loss 56.37525649040271\n",
      "Epoch 4: loss 2.3032712228001118\n",
      "Epoch 5: loss 2.3032932388135037\n",
      "Accuracy for member 6: 9.95%\n",
      "Epoch 1: loss 2.4850255129055476\n",
      "Epoch 2: loss 2.0917833628365026\n",
      "Epoch 3: loss 1.92699680617823\n",
      "Epoch 4: loss 1.8148282377864606\n",
      "Epoch 5: loss 1.7502949313995557\n",
      "Accuracy for member 7: 47.11%\n",
      "Epoch 1: loss 7.137467203048852\n",
      "Epoch 2: loss 2.3084091469883536\n",
      "Epoch 3: loss 2.309050904295315\n",
      "Epoch 4: loss 2.3090360850190965\n",
      "Epoch 5: loss 2.3103168978096957\n",
      "Accuracy for member 8: 9.56%\n",
      "Epoch 1: loss 98.32561548220845\n",
      "Epoch 2: loss 75.42989580928327\n",
      "Epoch 3: loss 68.3579491990062\n",
      "Epoch 4: loss 66.85966788465603\n",
      "Epoch 5: loss 71.40288276794239\n",
      "Accuracy for member 9: 10.16%\n",
      "Epoch 1: loss 3.9717349648094786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4825/3701341169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4825/3458874573.py\u001b[0m in \u001b[0;36mselect_best_model\u001b[0;34m(initial_train_data)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Evaluate the initial population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mbest_member\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_population\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Run the genetic algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4825/1342542954.py\u001b[0m in \u001b[0;36mevaluate_population\u001b[0;34m(members, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3099\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "  \n",
    "from torchvision import datasets, transforms\n",
    "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "\n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
    "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "##### Cifar Data\n",
    "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "    \n",
    "#We need two copies of this due to weird dataset api \n",
    "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
    "    \n",
    "\n",
    "accs = []\n",
    "\n",
    "for seed in range(1, 5):\n",
    "  prng = RandomState(seed)\n",
    "  random_permute = prng.permutation(np.arange(0, 1000))\n",
    "  classes =  prng.permutation(np.arange(0,10))\n",
    "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
    "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
    "\n",
    "\n",
    "  train_data = Subset(cifar_data, indx_train)\n",
    "  val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "  \n",
    "  train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=128, \n",
    "                                             shuffle=True)\n",
    "\n",
    "  val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=False)\n",
    "  \n",
    "\n",
    "\n",
    "  best_model = select_best_model(train_data)\n",
    "  input_size = (3,32,32)\n",
    "  y = calculate_size(best_model[0], best_model[1], best_model[2], best_model[3], best_model[4], input_size, best_model[8], best_model[10])\n",
    "  model = EvoDCNN(best_model[0], best_model[1], best_model[2], best_model[3], best_model[4], y, best_model[7], best_model[8], best_model[9], best_model[10])\n",
    "  model.to(device)\n",
    "  optimizer = best_model[6](model.parameters(), lr=best_model[5])\n",
    "\n",
    "  for epoch in range(100):\n",
    "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
    "    \n",
    "  accs.append(test(model, device, val_loader))\n",
    "\n",
    "accs = np.array(accs)\n",
    "print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
