{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV6DxGkPeQDD"
      },
      "source": [
        "***Challenge 1***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfgxjTd_lk"
      },
      "source": [
        "Here the goal is to train on 25 samples. In this preliminary testbed the evaluation will be done on a 2000 sample validation set. Note in the end the final evaluation will be done on the full CIFAR-10 test set as well as potentially a separate dataset. The validation samples here should not be used for training in any way, the final evaluation will provide only random samples of 25 from a datasource that is not the CIFAR-10 training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk0Ilt_-duk2"
      },
      "source": [
        "Feel free to modify this testbed to your liking, including the normalization transformations etc. Note however the final evaluation testbed will have a rigid set of components where you will need to place your answer. The only constraint is the data. Refer to the full project instructions for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyBTUe3idZI"
      },
      "source": [
        "Setup training functions. Again you are free to fully modify this testbed in your prototyping within the constraints of the data used. You can use tools outside of pytorch for training models if desired as well although the torchvision dataloaders will still be useful for interacting with the cifar-10 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7soYNWEedl9"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4hpe7QbQFnr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(3,64,kernel_size=3,padding=0),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,64,kernel_size=3,padding=0),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU())\n",
        "\n",
        "    def forward(self,x):\n",
        "        if len(x.shape)<4:\n",
        "          # print('TRUE')\n",
        "          x  = x.unsqueeze(0)\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        #out = out.view(out.size(0),-1)\n",
        "        return out # 64\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self,input_size,hidden_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        # nn.Conv2d(64*2,64,kernel_size=3,padding=0),\n",
        "                        nn.Conv2d(3,64,kernel_size=3,padding=0),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
        "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        \n",
        "\n",
        "        \n",
        "# self.fc1 = nn.Linear(128*2*2,hidden_size)\n",
        "        \n",
        "      \n",
        "        # self.fc1 = nn.Linear(input_size*3*3,hidden_size)\n",
        "        # self.fc1 = nn.Linear(2304,hidden_size)\n",
        "        # self.fc1 = nn.Linear(64,hidden_size)\n",
        "        # self.fc1 = nn.Linear (10816,hidden_size)\n",
        "        self.fc1 = nn.Linear(3136,hidden_size)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size,10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if len(x.shape)<4:\n",
        "          # print('TRUE')\n",
        "          x  = x.unsqueeze(0)\n",
        "        out = self.layer1(x)\n",
        "        # print(f'dim layer 1 = {out.shape}')\n",
        "        out = self.layer2(out)\n",
        "        # print(f'dim layer 2 = {out.shape}')\n",
        "#\n",
        "        # out = self.layer3(out)\n",
        "        # print(f'dim layer 3 = {out.shape}')\n",
        "        \n",
        "        out = out.view(out.size(0),-1)\n",
        "        # print(f'dim flatten = {out.shape}')\n",
        "        out = F.relu(self.fc1(out))\n",
        "        # print(f'dim fc1 = {out.shape}')\n",
        "\n",
        "        out = F.sigmoid(self.fc2(out))\n",
        "        return out\n",
        "\n",
        "class CNNEncoderNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CNNEncoderNet, self).__init__()\n",
        "        self.cnn_encoder = CNNEncoder()\n",
        "        self.net = Net(input_size, hidden_size)\n",
        "        self.cnn_encoder.to(device)\n",
        "        self.net.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn_encoder(x)\n",
        "        out = torch.cat((out, out), dim=1)  # concatenate the output of the CNN encoder\n",
        "        out = self.net(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPjWBE4MerTX"
      },
      "source": [
        "The below tries  2 random problem instances. In your development you may choose to prototype with 1 problem instances but keep in mind for small sample problems the variance is high so continously evaluating on several subsets will be important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7xU1HMelJ3",
        "outputId": "6ccbb369-1a28-446c-a273-5d568798ffc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.253584\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.815078\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.623162\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.600276\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598444\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598152\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598074\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598045\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598031\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598025\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598021\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598019\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598017\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598017\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598014\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 216/400 (54.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.325353\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.811795\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.621774\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.599800\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598290\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598081\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598031\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598014\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598000\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.597999\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.597999\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.597999\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.597998\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.597998\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.597998\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.597999\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.597998\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 202/400 (50.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.279454\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.852705\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.635064\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.601258\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598503\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598137\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598054\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598027\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598010\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598007\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598004\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 261/400 (65.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.326344\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.814569\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.623874\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.600091\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598343\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598101\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598042\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598021\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598008\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598003\n",
            "\n",
            "Test set: Average loss: 1.5981, Accuracy: 197/400 (49.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.290036\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.854222\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.637930\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.602860\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598952\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598303\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598138\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598081\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598057\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598045\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598038\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598034\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598032\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598031\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598030\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598029\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598029\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598028\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598028\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598027\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 206/400 (51.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.265078\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.820648\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.624984\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.600035\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598322\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598092\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598037\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598017\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598003\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598002\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598002\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598001\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598001\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.266683\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.814525\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.628904\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.601844\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598808\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598271\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598128\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598076\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598054\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598043\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598037\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598033\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598031\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598029\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598028\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598027\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598027\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598026\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598026\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598026\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 321/400 (80.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.271172\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.794370\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.619060\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.599771\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598313\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598097\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598042\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598022\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598013\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598007\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598004\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 209/400 (52.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.245243\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.812117\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.625929\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.600773\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598490\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598148\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598064\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598035\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598022\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598013\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598011\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598010\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598010\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598009\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 224/400 (56.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.281916\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.793817\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.620454\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.600149\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598383\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598117\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598050\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598027\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598017\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598011\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598007\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598007\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598006\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 200/400 (50.00%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.313410\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.849647\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.633327\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.601318\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598562\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598168\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598073\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598041\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598027\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598020\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598014\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598013\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598013\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598012\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598012\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 322/400 (80.50%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.341645\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.808585\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.620065\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.599950\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598343\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598104\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598044\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598022\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598013\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598009\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598006\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598005\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598004\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598004\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 277/400 (69.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.304916\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.866613\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.637837\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.601398\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598565\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598172\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598077\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598044\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598031\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598024\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598020\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598018\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598017\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598016\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598015\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598015\n",
            "\n",
            "Test set: Average loss: 1.5981, Accuracy: 265/400 (66.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.283465\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.838953\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 1.634214\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 1.602180\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 1.598819\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 1.598271\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 1.598130\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 1.598080\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 1.598058\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 1.598047\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 1.598041\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 1.598037\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 1.598035\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 1.598034\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 1.598033\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 1.598032\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 1.598032\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 1.598032\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 1.598032\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 1.598032\n",
            "\n",
            "Test set: Average loss: 1.5980, Accuracy: 207/400 (51.75%)\n",
            "\n",
            "Acc over 15 instances: 60.43 +- 10.77\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = 'cpu'\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api \n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "\n",
        "accs = []\n",
        "\n",
        "for seed in range(1, 15):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=64, \n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=False)\n",
        "  \n",
        "  input_size,hidden_size = 16,256\n",
        "  model = Net(input_size,hidden_size)\n",
        "  # model = CNNEncoderNet (input_size,hidden_size)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "  # optimizer = torch.optim.Adam(model.parameters(),lr=0.005,)\n",
        "\n",
        "  for epoch in range(100):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 15 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8GlJkOdfYY0"
      },
      "source": [
        "***Challenge 2***\n",
        "\n",
        "You may use the same testbed but without the constraints on external datasets or models trained on exeternal datasets. You may not however use any of the CIFAR-10 training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dktQa6mWiHYZ",
        "outputId": "2dbf2f22-d306-47bb-c698-c48622b32831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.347654\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.181205\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.983521\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 1.834520\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 1.743644\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.680331\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 1.636407\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 1.613036\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 1.603596\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 1.600180\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([16, 10816])\n",
            "\n",
            "Test set: Average loss: 1.5992, Accuracy: 201/400 (50.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.295530\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.147074\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.961580\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 1.822097\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 1.734057\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.672440\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 1.632241\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 1.611838\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 1.603391\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 1.600150\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([16, 10816])\n",
            "\n",
            "Test set: Average loss: 1.5991, Accuracy: 311/400 (77.75%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.273004\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.081017\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.887636\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 1.774948\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 1.702310\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.651680\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 1.621317\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 1.607161\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 1.601581\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 1.599466\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([16, 10816])\n",
            "\n",
            "Test set: Average loss: 1.5987, Accuracy: 235/400 (58.75%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.306696\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.155967\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 2 [0/50 (0%)]\tLoss: 1.972450\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 3 [0/50 (0%)]\tLoss: 1.847140\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 4 [0/50 (0%)]\tLoss: 1.763324\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 1.691816\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 6 [0/50 (0%)]\tLoss: 1.639638\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 7 [0/50 (0%)]\tLoss: 1.613488\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 8 [0/50 (0%)]\tLoss: 1.603434\n",
            "dim flatten = torch.Size([50, 10816])\n",
            "Train Epoch: 9 [0/50 (0%)]\tLoss: 1.599939\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([64, 10816])\n",
            "dim flatten = torch.Size([16, 10816])\n",
            "\n",
            "Test set: Average loss: 1.5989, Accuracy: 200/400 (50.00%)\n",
            "\n",
            "Acc over 2 instances: 59.19 +- 11.28\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "from numpy.random import RandomState\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "  \n",
        "from torchvision import datasets, transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # loss1 = F.cross_entropy(output[0], target)\n",
        "        # loss2 = F.cross_entropy(output[1], target)\n",
        "        # loss3 = F.cross_entropy(output[2], target)\n",
        "        # loss = loss1+loss2+loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "# We resize images to allow using imagenet pre-trained models, is there a better way?\n",
        "resize = transforms.Resize(224) \n",
        "\n",
        "transform_val = transforms.Compose([resize, transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "transform_train = transforms.Compose([resize, transforms.ToTensor(), normalize]) \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device) # you will really need gpu's for this part\n",
        "    \n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
        "    \n",
        "#We need two copies of this due to weird dataset api (lol)\n",
        "cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n",
        "    \n",
        "accs = []\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 5000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                            #  batch_size=128, \n",
        "                                             batch_size = 64,  # For VGG\n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                          #  batch_size=128, \n",
        "                                           batch_size = 64, # For VGG\n",
        "                                           shuffle=False)\n",
        "  \n",
        "  model = CNNEncoderNet (16,128)\n",
        "\n",
        "  # model = models.alexnet(pretrained=True)\n",
        "  # model = models.resnet50(pretrained=True)\n",
        "  # model = models.vgg19( ) # X\n",
        "  # model = models.GoogLeNet()\n",
        "  # model = models.vit_l_32(weights = 'IMAGENET1K_V1')\n",
        "\n",
        "\n",
        "  # model.classifier = nn.Linear(256 * 6 * 6, 10) #alexnet\n",
        "  \n",
        "  # num_ftrs = model.fc.in_features\n",
        "  # model.fc = nn.Linear(num_ftrs, 10) # resnet & GoogleNet\n",
        "\n",
        "  # num_features = model.classifier[-1].in_features\n",
        "  # model.classifier[-1] = nn.Linear(num_features, 10)\n",
        "\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), \n",
        "                              lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "\n",
        "\n",
        "  # optimizer = torch.optim.SGD(model.classifier.parameters(), \n",
        "  #                             lr=0.01, momentum=0.9,\n",
        "  #                             weight_decay=0.0005)\n",
        "\n",
        "  # optimizer = torch.optim.SGD(model.fc.parameters(), # for resnet Googlenet\n",
        "  #                           lr=0.01, momentum=0.9,\n",
        "  #                           weight_decay=0.0005)\n",
        "\n",
        "  # optimizer = torch.optim.SGD(model.classifier[-1].parameters(), # for resnet Googlenet\n",
        "  #                         lr=0.01, momentum=0.9,\n",
        "  #                         weight_decay=0.0005)\n",
        "\n",
        "  # optimizer = torch.optim.Adam(model.classifier.parameters())\n",
        "  # optimizer = torch.optim.Adam(model.parameters())\n",
        "  model.to(device)\n",
        "  for epoch in range(10):\n",
        "    train(model, device, train_loader, optimizer, epoch, display=True)\n",
        "    \n",
        "  accs.append(test(model, device, val_loader))\n",
        "\n",
        "accs = np.array(accs)\n",
        "print('Acc over 2 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}