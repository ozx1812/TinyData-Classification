{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.FileHandler(f'training.log')\n",
    "# Create STDERR handler\n",
    "# handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWaV: unsupervised training using clustering and swap prediction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = [\"downsample\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = [\"downsample\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block,\n",
    "            layers,\n",
    "            zero_init_residual=False,\n",
    "            groups=1,\n",
    "            widen=1,\n",
    "            width_per_group=64,\n",
    "            replace_stride_with_dilation=None,\n",
    "            norm_layer=None,\n",
    "            normalize=False,\n",
    "            output_dim=0,\n",
    "            hidden_mlp=0,\n",
    "            nmb_prototypes=0,\n",
    "            eval_mode=False,\n",
    "    ):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.eval_mode = eval_mode\n",
    "        self.padding = nn.ConstantPad2d(1, 0.0)\n",
    "\n",
    "        self.inplanes = width_per_group * widen\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        # change padding 3 -> 2 compared to original torchvision code because added a padding layer\n",
    "        num_out_filters = width_per_group * widen\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, num_out_filters, kernel_size=7, stride=2, padding=2, bias=False\n",
    "        )\n",
    "        self.bn1 = norm_layer(num_out_filters)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, num_out_filters, layers[0])\n",
    "        num_out_filters *= 2\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, num_out_filters, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
    "        )\n",
    "        num_out_filters *= 2\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, num_out_filters, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
    "        )\n",
    "        num_out_filters *= 2\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, num_out_filters, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # normalize output features\n",
    "        self.l2norm = normalize\n",
    "\n",
    "        # projection head\n",
    "        if output_dim == 0:\n",
    "            self.projection_head = None\n",
    "        elif hidden_mlp == 0:\n",
    "            self.projection_head = nn.Linear(num_out_filters * block.expansion, output_dim)\n",
    "        else:\n",
    "            self.projection_head = nn.Sequential(\n",
    "                nn.Linear(num_out_filters * block.expansion, hidden_mlp),\n",
    "                nn.BatchNorm1d(hidden_mlp),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_mlp, output_dim),\n",
    "            )\n",
    "\n",
    "        # prototype layer\n",
    "        self.prototypes = None\n",
    "        if isinstance(nmb_prototypes, list):\n",
    "            self.prototypes = MultiPrototypes(output_dim, nmb_prototypes)\n",
    "        elif nmb_prototypes > 0:\n",
    "            self.prototypes = nn.Linear(output_dim, nmb_prototypes, bias=False)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes,\n",
    "                planes,\n",
    "                stride,\n",
    "                downsample,\n",
    "                self.groups,\n",
    "                self.base_width,\n",
    "                previous_dilation,\n",
    "                norm_layer,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        x = self.padding(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.eval_mode:\n",
    "            return x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        if self.projection_head is not None:\n",
    "            x = self.projection_head(x)\n",
    "\n",
    "        if self.l2norm:\n",
    "            x = nn.functional.normalize(x, dim=1, p=2)\n",
    "\n",
    "        if self.prototypes is not None:\n",
    "            return x, self.prototypes(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            inputs = [inputs]\n",
    "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in inputs]),\n",
    "            return_counts=True,\n",
    "        )[1], 0)\n",
    "        start_idx = 0\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]))\n",
    "            # .to(device)\n",
    "                                        #  cuda(non_blocking=True))\n",
    "            if start_idx == 0:\n",
    "                output = _out\n",
    "            else:\n",
    "                output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "        return self.forward_head(output)\n",
    "\n",
    "\n",
    "class MultiPrototypes(nn.Module):\n",
    "    def __init__(self, output_dim, nmb_prototypes):\n",
    "        super(MultiPrototypes, self).__init__()\n",
    "        self.nmb_heads = len(nmb_prototypes)\n",
    "        for i, k in enumerate(nmb_prototypes):\n",
    "            self.add_module(\"prototypes\" + str(i), nn.Linear(output_dim, k, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for i in range(self.nmb_heads):\n",
    "            out.append(getattr(self, \"prototypes\" + str(i))(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "\n",
    "\n",
    "def resnet50w2(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=2, **kwargs)\n",
    "\n",
    "\n",
    "def resnet50w4(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=4, **kwargs)\n",
    "\n",
    "\n",
    "def resnet50w5(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=5, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_mlp = 2048\n",
    "feat_dim = 128\n",
    "nmb_prototypes = 3000 # default was 3000\n",
    "\n",
    "model = resnet50(normalize=True,\n",
    "        hidden_mlp=hidden_mlp,\n",
    "        output_dim=feat_dim,\n",
    "        nmb_prototypes=nmb_prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import ImageFilter, Image\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MultiCropCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        size_crops,\n",
    "        nmb_crops,\n",
    "        min_scale_crops,\n",
    "        max_scale_crops,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        size_dataset=-1,\n",
    "        return_index=False,\n",
    "        download=True\n",
    "    ):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform, download=download)\n",
    "        assert len(size_crops) == len(nmb_crops)\n",
    "        assert len(min_scale_crops) == len(nmb_crops)\n",
    "        assert len(max_scale_crops) == len(nmb_crops)\n",
    "        if size_dataset >= 0:\n",
    "            self.data = self.data[:size_dataset]\n",
    "            self.targets = self.targets[:size_dataset]\n",
    "        self.return_index = return_index\n",
    "\n",
    "        color_transform = [get_color_distortion(), PILRandomGaussianBlur()]\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.228, 0.224, 0.225]\n",
    "        trans = []\n",
    "        for i in range(len(size_crops)):\n",
    "            randomresizedcrop = transforms.RandomResizedCrop(\n",
    "                size_crops[i],\n",
    "                scale=(min_scale_crops[i], max_scale_crops[i]),\n",
    "            )\n",
    "            trans.extend([transforms.Compose([\n",
    "                randomresizedcrop,\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Compose(color_transform),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)])\n",
    "            ] * nmb_crops[i])\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(self.data[index])\n",
    "        multi_crops = list(map(lambda trans: trans(image), self.trans))\n",
    "        target = self.targets[index]\n",
    "        if self.return_index:\n",
    "            return index, multi_crops\n",
    "        return multi_crops\n",
    "    \n",
    "\n",
    "class MultiCropSVHN(torchvision.datasets.SVHN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        size_crops,\n",
    "        nmb_crops,\n",
    "        min_scale_crops,\n",
    "        max_scale_crops,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        size_dataset=-1,\n",
    "        return_index=False,\n",
    "        download=True\n",
    "    ):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform, download=download)\n",
    "        assert len(size_crops) == len(nmb_crops)\n",
    "        assert len(min_scale_crops) == len(nmb_crops)\n",
    "        assert len(max_scale_crops) == len(nmb_crops)\n",
    "        if size_dataset >= 0:\n",
    "            self.data = self.data[:size_dataset]\n",
    "            self.targets = self.labels[:size_dataset]\n",
    "        self.return_index = return_index\n",
    "\n",
    "        color_transform = [get_color_distortion(), PILRandomGaussianBlur()]\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.228, 0.224, 0.225]\n",
    "        trans = []\n",
    "        for i in range(len(size_crops)):\n",
    "            randomresizedcrop = transforms.RandomResizedCrop(\n",
    "                size_crops[i],\n",
    "                scale=(min_scale_crops[i], max_scale_crops[i]),\n",
    "            )\n",
    "            trans.extend([transforms.Compose([\n",
    "                randomresizedcrop,\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Compose(color_transform),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)])\n",
    "            ] * nmb_crops[i])\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(self.data[index])\n",
    "        multi_crops = list(map(lambda trans: trans(image), self.trans))\n",
    "        target = int(self.targets[index])\n",
    "        if self.return_index:\n",
    "            return index, multi_crops\n",
    "        return multi_crops\n",
    "\n",
    "class MultiCropSTL10(torchvision.datasets.STL10):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        size_crops,\n",
    "        nmb_crops,\n",
    "        min_scale_crops,\n",
    "        max_scale_crops,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        size_dataset=-1,\n",
    "        return_index=False,\n",
    "        download=True\n",
    "    ):\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform, download=download)\n",
    "        assert len(size_crops) == len(nmb_crops)\n",
    "        assert len(min_scale_crops) == len(nmb_crops)\n",
    "        assert len(max_scale_crops) == len(nmb_crops)\n",
    "        if size_dataset >= 0:\n",
    "            self.data = self.data[:size_dataset]\n",
    "            self.targets = self.labels[:size_dataset]\n",
    "        self.return_index = return_index\n",
    "\n",
    "        color_transform = [get_color_distortion(), PILRandomGaussianBlur()]\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.228, 0.224, 0.225]\n",
    "        trans = []\n",
    "        for i in range(len(size_crops)):\n",
    "            randomresizedcrop = transforms.RandomResizedCrop(\n",
    "                size_crops[i],\n",
    "                scale=(min_scale_crops[i], max_scale_crops[i]),\n",
    "            )\n",
    "            trans.extend([transforms.Compose([\n",
    "                randomresizedcrop,\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Compose(color_transform),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)])\n",
    "            ] * nmb_crops[i])\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(np.transpose(self.data[index], (1, 2, 0)))\n",
    "        multi_crops = list(map(lambda trans: trans(image), self.trans))\n",
    "        target = self.targets[index]\n",
    "        if self.return_index:\n",
    "            return index, multi_crops\n",
    "        return multi_crops\n",
    "\n",
    "\n",
    "\n",
    "class PILRandomGaussianBlur(object):\n",
    "    \"\"\"\n",
    "    Apply Gaussian Blur to the PIL image. Take the radius and probability of\n",
    "    application as the parameter.\n",
    "    This transform was used in SimCLR - https://arxiv.org/abs/2002.05709\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n",
    "        self.prob = p\n",
    "        self.radius_min = radius_min\n",
    "        self.radius_max = radius_max\n",
    "\n",
    "    def __call__(self, img):\n",
    "        do_it = np.random.rand() <= self.prob\n",
    "        if not do_it:\n",
    "            return img\n",
    "\n",
    "        return img.filter(\n",
    "            ImageFilter.GaussianBlur(\n",
    "                radius=random.uniform(self.radius_min, self.radius_max)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def get_color_distortion(s=1.0):\n",
    "    # s is the strength of color distortion.\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: image_datasets/train_32x32.mat\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the parameters for the multi-crop dataset\n",
    "data_path = 'image_datasets'\n",
    "size_crops = [224, 96]\n",
    "nmb_crops = [2, 6]\n",
    "min_scale_crops = [0.25, 0.05]\n",
    "max_scale_crops = [1.0, 0.3]\n",
    "size_dataset = 100  # number of images to use in the dataset\n",
    "return_index = False\n",
    "\n",
    "# Create the multi-crop dataset\n",
    "dataset = MultiCropCIFAR10(\n",
    "    data_path,\n",
    "    size_crops,\n",
    "    nmb_crops,\n",
    "    min_scale_crops,\n",
    "    max_scale_crops,\n",
    "    size_dataset=size_dataset,\n",
    "    return_index=return_index,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Create a data loader for the dataset\n",
    "batch_size = 4\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "dataset = MultiCropSVHN(\n",
    "    data_path,\n",
    "    size_crops,\n",
    "    nmb_crops,\n",
    "    min_scale_crops,\n",
    "    max_scale_crops,\n",
    "    size_dataset=size_dataset,\n",
    "    return_index=return_index,\n",
    "    download=True\n",
    ")\n",
    "batch_size = 256\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "dataset = MultiCropSTL10(\n",
    "    data_path,\n",
    "    size_crops,\n",
    "    nmb_crops,\n",
    "    min_scale_crops,\n",
    "    max_scale_crops,\n",
    "    size_dataset=size_dataset,\n",
    "    return_index=return_index,\n",
    "    download=True\n",
    ")\n",
    "batch_size = 4\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# directory_path = \"./challange2\"\n",
    "# os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "# handler = logging.FileHandler('training-challange-2.log')\n",
    "\n",
    "# # Create formatter and add it to the handler\n",
    "# formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "\n",
    "# # Set STDERR handler as the only handler \n",
    "# logger.handlers = [handler]\n",
    "\n",
    "# use_fp16 = False\n",
    "# dump_path = \"./challange2\"\n",
    "# rank = 0\n",
    "# world_size = 1\n",
    "# epoch_queue_starts = 15\n",
    "# crops_for_assign = [0,1]\n",
    "# checkpoint_freq = 25\n",
    "# temperature = 0.1\n",
    "# freeze_prototypes_niters = 313\n",
    "# epsilon = 0.05\n",
    "# sinkhorn_iterations = 3\n",
    "# epochs = 100\n",
    "\n",
    "def fix_random_seeds(seed=31):\n",
    "    \"\"\"\n",
    "    Fix random seeds.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def train_pretrained_util(train_loader, model, optimizer, epoch, lr_schedule, queue, logger):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    crops_for_assign = [0,1]\n",
    "    temperature = 0.1\n",
    "    freeze_prototypes_niters = 313\n",
    "    rank = 0\n",
    "\n",
    "    model.train()\n",
    "    use_the_queue = False\n",
    "\n",
    "    end = time.time()\n",
    "    for it, inputs in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # update learning rate\n",
    "        iteration = epoch * len(train_loader) + it\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr_schedule[iteration]\n",
    "\n",
    "        # normalize the prototypes\n",
    "        with torch.no_grad():\n",
    "            w = model.prototypes.weight.data.clone()\n",
    "            w = nn.functional.normalize(w, dim=1, p=2)\n",
    "            model.prototypes.weight.copy_(w)\n",
    "\n",
    "        # ============ multi-res forward passes ... ============\n",
    "        # print(inputs[0].shape)\n",
    "        embedding, output = model(inputs)\n",
    "        embedding = embedding.detach()\n",
    "        bs = inputs[0].size(0)\n",
    "\n",
    "        # ============ swav loss ... ============\n",
    "        loss = 0\n",
    "        for i, crop_id in enumerate(crops_for_assign):\n",
    "            with torch.no_grad():\n",
    "                out = output[bs * crop_id: bs * (crop_id + 1)].detach()\n",
    "\n",
    "                # time to use the queue\n",
    "                if queue is not None:\n",
    "                    if use_the_queue or not torch.all(queue[i, -1, :] == 0):\n",
    "                        use_the_queue = True\n",
    "                        out = torch.cat((torch.mm(\n",
    "                            queue[i],\n",
    "                            model.prototypes.weight.t()\n",
    "                        ), out))\n",
    "                    # fill the queue\n",
    "                    queue[i, bs:] = queue[i, :-bs].clone()\n",
    "                    queue[i, :bs] = embedding[crop_id * bs: (crop_id + 1) * bs]\n",
    "\n",
    "                # get assignments\n",
    "                q = distributed_sinkhorn(out)[-bs:]\n",
    "\n",
    "            # cluster assignment prediction\n",
    "            subloss = 0\n",
    "            for v in np.delete(np.arange(np.sum(nmb_crops)), crop_id):\n",
    "                x = output[bs * v: bs * (v + 1)] / temperature\n",
    "                subloss -= torch.mean(torch.sum(q * F.log_softmax(x, dim=1), dim=1))\n",
    "            loss += subloss / (np.sum(nmb_crops) - 1)\n",
    "        loss /= len(crops_for_assign)\n",
    "\n",
    "        # ============ backward and optim step ... ============\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # cancel gradients for the prototypes\n",
    "        if iteration < freeze_prototypes_niters:\n",
    "            for name, p in model.named_parameters():\n",
    "                if \"prototypes\" in name:\n",
    "                    p.grad = None\n",
    "        optimizer.step()\n",
    "\n",
    "        # ============ misc ... ============\n",
    "        losses.update(loss.item(), inputs[0].size(0))\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if rank ==0 and it % 50 == 0:\n",
    "            logger.info(\n",
    "                \"Epoch: [{0}][{1}]\\t\"\n",
    "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "                \"Lr: {lr:.4f}\".format(\n",
    "                    epoch,\n",
    "                    it,\n",
    "                    batch_time=batch_time,\n",
    "                    data_time=data_time,\n",
    "                    loss=losses,\n",
    "                    lr=optimizer.param_groups[0][\"lr\"],\n",
    "                )\n",
    "            )\n",
    "    return (epoch, losses.avg), queue\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def distributed_sinkhorn(out, epsilon=0.05, world_size=1, sinkhorn_iterations=3):\n",
    "    Q = torch.exp(out / epsilon).t() # Q is K-by-B for consistency with notations from our paper\n",
    "    B = Q.shape[1] * world_size # number of samples to assign\n",
    "    K = Q.shape[0] # how many prototypes\n",
    "\n",
    "    # make the matrix sums to 1\n",
    "    sum_Q = torch.sum(Q)\n",
    "    # dist.all_reduce(sum_Q)\n",
    "    Q /= sum_Q\n",
    "\n",
    "    for it in range(sinkhorn_iterations):\n",
    "        # normalize each row: total weight per prototype must be 1/K\n",
    "        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n",
    "        # dist.all_reduce(sum_of_rows)\n",
    "        Q /= sum_of_rows\n",
    "        Q /= K\n",
    "\n",
    "        # normalize each column: total weight per sample must be 1/B\n",
    "        Q /= torch.sum(Q, dim=0, keepdim=True)\n",
    "        Q /= B\n",
    "\n",
    "    Q *= B # the colomns must sum to 1 so that Q is an assignment\n",
    "    return Q.t()\n",
    "\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_finetuned_util(model, device, train_loader, optimizer, epoch, logger, display=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "        logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test_finetuned_util(model, device, test_loader, logger):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class FineTunedSwav(nn.Module):\n",
    "    def __init__(self, pre_trained, num_classes) -> None:\n",
    "        super(FineTunedSwav, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        model_children = list(pre_trained.children())[:-2]\n",
    "        self.pre_trained = nn.Sequential(*model_children)\n",
    "        \n",
    "        # Replace the last two layers with a single fully connected layer\n",
    "        last_layer_in_features = pre_trained.projection_head[0].in_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(last_layer_in_features, self.num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pre_trained(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_finetuned(backbone_model, config, logger):\n",
    "    f_config = config[\"fine_tuned\"]\n",
    "\n",
    "    crop = transforms.RandomResizedCrop(224)\n",
    "    hflip = transforms.RandomHorizontalFlip()\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        crop,\n",
    "        transforms.RandomApply([hflip], p=0.5),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "    # We resize images to allow using imagenet pre-trained models, is there a better way?\n",
    "    resize = transforms.Resize(224) \n",
    "\n",
    "    transform_val = transforms.Compose([resize, transforms.ToTensor(), transform]) #careful to keep this one same\n",
    "    transform_train = transforms.Compose([resize, transforms.ToTensor(), transform]) \n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(device) # you will really need gpu's for this part\n",
    "\n",
    "\n",
    "    ##### Cifar Data\n",
    "    cifar_data = datasets.CIFAR10(root=f_config['dataset_path'],train=True, transform=transform_train, download=True)\n",
    "        \n",
    "    #We need two copies of this due to weird dataset api \n",
    "    cifar_data_val = datasets.CIFAR10(root=f_config['dataset_path'],train=True, transform=transform_val, download=True)\n",
    "        \n",
    "    accs = []\n",
    "\n",
    "    for seed in range(1, 5):\n",
    "        prng = RandomState(seed)\n",
    "        random_permute = prng.permutation(np.arange(0, 5000))\n",
    "        classes =  prng.permutation(np.arange(0,10))\n",
    "        indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
    "        indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
    "\n",
    "        train_data = Subset(cifar_data, indx_train)\n",
    "        val_data = Subset(cifar_data_val, indx_val)\n",
    "\n",
    "        print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "        logger.info('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                                    batch_size=f_config['batch_size'], \n",
    "                                                    shuffle=True)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                                batch_size=f_config['batch_size'], \n",
    "                                                shuffle=False)\n",
    "\n",
    "        # ORIGINAL #\n",
    "        # model = models.alexnet(pretrained=True)\n",
    "        # model.classifier = nn.Linear(256 * 6 * 6, 10)\n",
    "        ############\n",
    "        model = FineTunedSwav(pre_trained=backbone_model, num_classes=f_config['num_classes'])\n",
    "    \n",
    "        optimizer = torch.optim.SGD(model.fc.parameters(), \n",
    "                                    lr=f_config['lr'], momentum=f_config['momentum'],\n",
    "                                    weight_decay=f_config['weight_decay'])\n",
    "\n",
    "        model.to(device)\n",
    "        for epoch in range(f_config['epochs']):\n",
    "            train_finetuned_util(model, device, train_loader, optimizer, epoch, logger=logger, display=epoch%(f_config['epochs']//10)==0)\n",
    "\n",
    "        accs.append(test_finetuned_util(model, device, val_loader, logger=logger))\n",
    "\n",
    "    accs = np.array(accs)\n",
    "    logger.info('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
    "    print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def train_pretrained(data_loader, config, logger):\n",
    "    device = config['device']\n",
    "    p_config = config['pre_trained']\n",
    "    # build model\n",
    "    hidden_mlp = p_config['hidden_mlp']\n",
    "    feat_dim = p_config['feat_dim']\n",
    "    nmb_prototypes = p_config['num_prototypes'] # default was 3000\n",
    "\n",
    "    model = resnet50(normalize=True,\n",
    "            hidden_mlp=hidden_mlp,\n",
    "            output_dim=feat_dim,\n",
    "            nmb_prototypes=nmb_prototypes)\n",
    "    \n",
    "    # synchronize batch norm layers\n",
    "    fix_random_seeds()\n",
    "\n",
    "    base_lr = p_config['base_lr']\n",
    "    wd = p_config['weight_decay']\n",
    "    mu = p_config['momentum']\n",
    "    \n",
    "    # build optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=base_lr,\n",
    "        momentum=mu,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    base_lr = p_config['base_lr']\n",
    "    wd = p_config['weight_decay']\n",
    "    mu = p_config['momentum']\n",
    "    start_warmup = p_config['start_warmup']\n",
    "    warmup_epochs = p_config['warmup_epochs']\n",
    "    final_lr = p_config['final_lr']\n",
    "    epochs = p_config['epochs']\n",
    "    warmup_lr_schedule = np.linspace(start_warmup, base_lr, len(data_loader) * warmup_epochs)\n",
    "    iters = np.arange(len(data_loader) * (epochs - warmup_epochs))\n",
    "    cosine_lr_schedule = np.array([final_lr + 0.5 * (base_lr - final_lr) * (1 + \\\n",
    "                         math.cos(math.pi * t / (len(data_loader) * (epochs - warmup_epochs)))) for t in iters])\n",
    "    lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
    "    logger.info(\"Building optimizer done.\")\n",
    "\n",
    "    dump_path = config['model_dir']\n",
    "    rank = 0\n",
    "    world_size = 1\n",
    "    epoch_queue_starts = p_config['epoch_queue_starts']\n",
    "    crops_for_assign = p_config['crops_for_assign']\n",
    "    # build the queue\n",
    "    queue_length = p_config['queue_length']\n",
    "    batch_size = p_config['batch_size']\n",
    "    queue = None\n",
    "    queue_path = os.path.join(dump_path, \"queue\" + str(rank) + \".pth\")\n",
    "    if os.path.isfile(queue_path):\n",
    "        queue = torch.load(queue_path)[\"queue\"]\n",
    "    # the queue needs to be divisible by the batch size\n",
    "    queue_length -= queue_length % (batch_size * world_size)\n",
    "\n",
    "    start_epoch = 0\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # train the network for one epoch\n",
    "        logger.info(\"============ Starting epoch %i ... ============\" % epoch)\n",
    "\n",
    "        # optionally starts a queue\n",
    "        if queue_length > 0 and epoch >= epoch_queue_starts and queue is None:\n",
    "            queue = torch.zeros(\n",
    "                len(crops_for_assign),\n",
    "                queue_length // world_size,\n",
    "                feat_dim,\n",
    "            ).to(device)\n",
    "\n",
    "        # train the network\n",
    "        scores, queue = train_pretrained_util(data_loader, model, optimizer, epoch, lr_schedule, queue, logger=logger)\n",
    "\n",
    "        # save checkpoints\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        chk_path = os.path.join(dump_path, \"checkpoint.pth.tar\")\n",
    "        torch.save(\n",
    "            save_dict,\n",
    "            chk_path,\n",
    "        )\n",
    "        if queue is not None:\n",
    "            torch.save({\"queue\": queue}, queue_path)\n",
    "    \n",
    "    return model, chk_path\n",
    "\n",
    "def load_pretrained(model, saved_chk_pt_path):\n",
    "    checkpoint = torch.load(saved_chk_pt_path)\n",
    "    checkpoint_key_model = 'state_dict'\n",
    "    checkpoint_key_optimizer = 'optimizer'\n",
    "    model_weights = checkpoint[checkpoint_key_model]\n",
    "    optimizer_state = checkpoint[checkpoint_key_optimizer]\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    hidden_mlp = 2048\n",
    "    feat_dim = 128\n",
    "    nmb_prototypes = 512 # default was 3000\n",
    "\n",
    "    model = resnet50(normalize=True,\n",
    "                hidden_mlp=hidden_mlp,\n",
    "                output_dim=feat_dim,\n",
    "                nmb_prototypes=nmb_prototypes)\n",
    "    model.load_state_dict(model_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_new_logger(f_path):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler(filename=f_path)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def pre_train_and_finetune(dataset_name:str = 'svhn'):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    config = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'log_dir': 'challange2-logs',\n",
    "        'device': device,\n",
    "        'pre_trained': {\n",
    "            'num_classes': 10,\n",
    "            'hidden_mlp': 2048,\n",
    "            'feat_dim': 128,\n",
    "            'num_prototypes': 32,\n",
    "            'batch_size': 256,\n",
    "            'data_path': 'image_datasets',\n",
    "            'size_crops': [224, 96],\n",
    "            'nmb_crops': [2, 6],\n",
    "            'min_scale_crops': [0.25, 0.05],\n",
    "            'max_scale_crops': [1.0, 3.0],\n",
    "            'size_dataset': 10000,\n",
    "            'base_lr': 4.8,\n",
    "            'weight_decay': 0.0005,\n",
    "            'momentum': 0.9,\n",
    "            'start_warmup': 0.0,\n",
    "            'warmup_epochs': 10,\n",
    "            'final_lr': 0.001,\n",
    "            'epochs': 5,\n",
    "            'epoch_queue_starts': 1,\n",
    "            'crops_for_assign': [0,1],\n",
    "            'queue_length': 1000\n",
    "        },\n",
    "        'fine_tuned': {\n",
    "            'lr': 0.001,\n",
    "            'dataset_path': 'image_datasets',\n",
    "            'num_classes': 10,\n",
    "            'epochs': 10,\n",
    "            'batch_size': 32,\n",
    "            'weight_decay': 0.0005,\n",
    "            'momentum': 0.9,\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    p_config = config['pre_trained']\n",
    "    f_config = config['fine_tuned']\n",
    "    # lr = f_config['lr']\n",
    "    \n",
    "    t_now = time.time()\n",
    "    experiment_dir = f'{config[\"dataset_name\"]}_{p_config[\"epochs\"]}_{f_config[\"epochs\"]}_{f_config[\"batch_size\"]}'\n",
    "    config['log_dir'] = os.path.join(experiment_dir, 'logs')\n",
    "    config['model_dir'] = os.path.join(experiment_dir, 'models')\n",
    "    f_name = f'{experiment_dir}_{t_now}__.log'\n",
    "    \n",
    "    log_dir = config['log_dir']\n",
    "    model_dir = config['model_dir']\n",
    "    f_path = os.path.join(log_dir, f_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    f_logger = get_new_logger(f_path)\n",
    "\n",
    "    # Define the parameters for the multi-crop dataset\n",
    "    p_config = config['pre_trained']\n",
    "    data_path = p_config[\"data_path\"]\n",
    "    size_crops = p_config['size_crops']\n",
    "    nmb_crops = p_config['nmb_crops']\n",
    "    min_scale_crops = p_config['min_scale_crops']\n",
    "    max_scale_crops = p_config['max_scale_crops']\n",
    "    batch_size = p_config['batch_size']\n",
    "    size_dataset = p_config['size_dataset']  # number of images to use in the dataset\n",
    "    return_index = False\n",
    "\n",
    "    dataset = None\n",
    "    data_loader = None\n",
    "    if dataset_name == 'svhn':\n",
    "        # get the dataset\n",
    "        dataset = MultiCropSVHN(\n",
    "            data_path,\n",
    "            size_crops,\n",
    "            nmb_crops,\n",
    "            min_scale_crops,\n",
    "            max_scale_crops,\n",
    "            size_dataset=size_dataset,\n",
    "            return_index=return_index,\n",
    "            download=True\n",
    "        )\n",
    "    \n",
    "    if dataset_name == 'stl10':\n",
    "        print('using stl10')\n",
    "        dataset = MultiCropSTL10(\n",
    "            data_path,\n",
    "            size_crops,\n",
    "            nmb_crops,\n",
    "            min_scale_crops,\n",
    "            max_scale_crops,\n",
    "            size_dataset=size_dataset,\n",
    "            return_index=return_index,\n",
    "            download=True\n",
    "        )\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    # train the model\n",
    "    model, saved_chk_pt_path = train_pretrained(data_loader, config, f_logger)\n",
    "    # p_config = config['pre_trained']\n",
    "    # # build model\n",
    "    # hidden_mlp = p_config['hidden_mlp']\n",
    "    # feat_dim = p_config['feat_dim']\n",
    "    # nmb_prototypes = p_config['num_prototypes'] # default was 3000\n",
    "\n",
    "    # model = resnet50(normalize=True,\n",
    "    #         hidden_mlp=hidden_mlp,\n",
    "    #         output_dim=feat_dim,\n",
    "    #         nmb_prototypes=nmb_prototypes)\n",
    "    # saved_chk_pt_path = os.path.join(model_dir, 'checkpoint.pth.tar')\n",
    "    # model = load_pretrained(model, saved_chk_pt_path)\n",
    "    train_finetuned(model, config, f_logger)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using stl10\n",
      "Files already downloaded and verified\n",
      "cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [18/50 (50%)]\tLoss: 2.292638\n",
      "Train Epoch: 1 [18/50 (50%)]\tLoss: 2.290266\n",
      "Train Epoch: 2 [18/50 (50%)]\tLoss: 2.277349\n",
      "Train Epoch: 3 [18/50 (50%)]\tLoss: 2.263239\n",
      "Train Epoch: 4 [18/50 (50%)]\tLoss: 2.247543\n",
      "Train Epoch: 5 [18/50 (50%)]\tLoss: 2.227833\n",
      "Train Epoch: 6 [18/50 (50%)]\tLoss: 2.230460\n",
      "Train Epoch: 7 [18/50 (50%)]\tLoss: 2.225876\n",
      "Train Epoch: 8 [18/50 (50%)]\tLoss: 2.214658\n",
      "Train Epoch: 9 [18/50 (50%)]\tLoss: 2.219043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Omij/DalTag/DalTag/backend/.venv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2571, Accuracy: 200/400 (50.00%)\n",
      "\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [18/50 (50%)]\tLoss: 2.285941\n",
      "Train Epoch: 1 [18/50 (50%)]\tLoss: 2.277781\n",
      "Train Epoch: 2 [18/50 (50%)]\tLoss: 2.283037\n",
      "Train Epoch: 3 [18/50 (50%)]\tLoss: 2.252053\n",
      "Train Epoch: 4 [18/50 (50%)]\tLoss: 2.241378\n",
      "Train Epoch: 5 [18/50 (50%)]\tLoss: 2.282335\n",
      "Train Epoch: 6 [18/50 (50%)]\tLoss: 2.224898\n",
      "Train Epoch: 7 [18/50 (50%)]\tLoss: 2.222885\n",
      "Train Epoch: 8 [18/50 (50%)]\tLoss: 2.214020\n",
      "Train Epoch: 9 [18/50 (50%)]\tLoss: 2.209086\n",
      "\n",
      "Test set: Average loss: 2.2376, Accuracy: 271/400 (67.75%)\n",
      "\n",
      "Num Samples For Training 50 Num Samples For Val 400\n",
      "Train Epoch: 0 [18/50 (50%)]\tLoss: 2.286137\n",
      "Train Epoch: 1 [18/50 (50%)]\tLoss: 2.305243\n",
      "Train Epoch: 2 [18/50 (50%)]\tLoss: 2.266268\n",
      "Train Epoch: 3 [18/50 (50%)]\tLoss: 2.258844\n",
      "Train Epoch: 4 [18/50 (50%)]\tLoss: 2.249844\n",
      "Train Epoch: 5 [18/50 (50%)]\tLoss: 2.240918\n"
     ]
    }
   ],
   "source": [
    "pre_train_and_finetune(dataset_name='stl10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
