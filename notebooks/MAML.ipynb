{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqQ5tBxc2/k2LwsQUiiP+p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bCTmM6rImCYL","executionInfo":{"status":"ok","timestamp":1682816721139,"user_tz":240,"elapsed":11950,"user":{"displayName":"Niki Monjazeb","userId":"10253055676513844231"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","\n","class MAMLLearner(nn.Module):\n","      def __init__(self):\n","        super(MAMLLearner, self).__init__()\n","        self.layers = nn.ModuleList()\n","\n","        self.layers+=[nn.Conv2d(3, 8,  kernel_size=3) , \n","                      nn.BatchNorm2d(8) ,\n","                      nn.ReLU(inplace=True)]\n","        self.layers+=[nn.Conv2d(8, 8,  kernel_size=3, stride=2), \n","                      nn.BatchNorm2d(8) ,\n","                      nn.ReLU(inplace=True)]\n","        self.layers+=[nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1), \n","                      nn.BatchNorm2d(16), \n","                      nn.ReLU(inplace=True)]\n","        self.layers+=[nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1), \n","                      nn.BatchNorm2d(16), \n","                      nn.ReLU(inplace=True)]\n","        self.layers+=[nn.Conv2d(16, 32,  kernel_size=3), \n","                      nn.BatchNorm2d(32) ,\n","                      nn.ReLU(inplace=True)]\n","        self.layers+=[nn.Conv2d(32, 32,  kernel_size=3, stride=2), \n","                      nn.BatchNorm2d(32) ,\n","                      nn.ReLU(inplace=True)]\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(32*5*5, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(128, 10)\n","        )\n","\n","        self.weight_decay = 0.01\n","\n","      def forward(self, x, params=None):\n","        if params is None:\n","            params = OrderedDict(self.named_parameters())\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc_layers(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","      def weight_regularization_loss(self):\n","        l2_reg = torch.tensor(0., requires_grad=True)\n","        for name, param in self.named_parameters():\n","            if 'bias' not in name:\n","                l2_reg = l2_reg + torch.norm(param, p=2)\n","        return self.weight_decay * l2_reg\n","        \n","      def meta_named_pars(self):\n","          for name, param in self.named_parameters():\n","              if param.requires_grad:\n","                  yield name, param\n","  \n","      def meta_params(self):\n","          return [p for _, p in self.meta_named_pars()]\n","        \n","  "]},{"cell_type":"code","source":["# Train without First-order approximation\n","def maml_train(model, device, train_tasks, test_tasks, optimizer, epoch, num_inner_updates=50, display=True):\n","    model.train()\n","    outer_loss = 0\n","\n","    for task_idx, (train_loader, test_loader) in enumerate(zip(train_tasks, test_tasks)):\n","        # Inner loop\n","        inner_optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","        for i in range(num_inner_updates):\n","            data, target = next(iter(train_loader))\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","            model.zero_grad()\n","            grads = torch.autograd.grad(loss, model.meta_params(), create_graph= True)\n","            params = OrderedDict()\n","            for (name, param), grad in zip(model.meta_named_pars(),grads):\n","              params[name] = param - 0.01 * grad.detach()\n","              params[name].retain_grad()\n","            # perform inner update\n","            inner_optimizer.zero_grad()\n","            loss.backward()\n","            for name, param in model.named_parameters():\n","                if name in params:\n","                    param.grad = params[name].grad\n","            inner_optimizer.step()\n","\n","        # Outer loop\n","        optimizer.zero_grad()\n","        test_data, test_target = next(iter(test_loader))\n","        test_data, test_target = test_data.to(device), test_target.to(device)\n","        test_output = model(test_data)\n","        outer_loss += F.cross_entropy(test_output, test_target)\n","    outer_loss.backward()\n","    optimizer.step()\n","\n","    if display:\n","        print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n","            epoch, outer_loss.item()))"],"metadata":{"id":"XoGBVhXPmJI2","executionInfo":{"status":"ok","timestamp":1682816791138,"user_tz":240,"elapsed":186,"user":{"displayName":"Niki Monjazeb","userId":"10253055676513844231"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Train with fFirst-order approximationFirst-order approximation\n","def maml_train_FOA(model, device, train_tasks, test_tasks, optimizer, epoch, num_inner_updates=50, display=True):\n","    model.train()\n","    outer_loss = 0\n","\n","    for task_idx, (train_loader, test_loader) in enumerate(zip(train_tasks, test_tasks)):\n","        # Inner loop\n","        inner_optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","        for i in range(num_inner_updates):\n","            data, target = next(iter(train_loader))\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","            model.zero_grad()\n","            loss.backward()\n","            inner_optimizer.step()\n","\n","        # Outer loop\n","        test_data, test_target = next(iter(test_loader))\n","        test_data, test_target = test_data.to(device), test_target.to(device)\n","\n","        # Compute meta-gradients\n","        model.zero_grad()\n","        output = model(test_data)\n","        outer_loss = F.cross_entropy(output, test_target)\n","        outer_loss.backward()\n","\n","        # Compute first-order approximation of meta-gradients\n","        meta_grads = []\n","        for param in model.parameters():\n","            meta_grads.append(param.grad.clone())\n","            param.grad.zero_()\n","\n","        # Update model parameters using meta-gradients\n","        with torch.no_grad():\n","            for param, meta_grad in zip(model.parameters(), meta_grads):\n","                param -= 0.01 * meta_grad\n","\n","        if display:\n","            print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, outer_loss.item()))\n","\n","    optimizer.step()\n"],"metadata":{"id":"ezZpbxOtmsEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def maml_test(model, device, test_tasks, num_inner_updates=5):\n","    model.eval()\n","    test_acc = []\n","\n","    for task_idx, test_loader in enumerate(test_tasks):\n","        # Inner loop\n","        inner_optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","        for i in range(num_inner_updates):\n","            data, target = next(iter(test_loader))\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","            model.zero_grad()\n","            grads = torch.autograd.grad(loss, model.meta_params(), create_graph=True)\n","            params = OrderedDict()\n","            for (name, param), grad in zip(model.meta_named_pars(), grads):\n","                params[name] = param - 0.01 * grad.detach()\n","                params[name].retain_grad()\n","\n","            # perform inner update\n","            inner_optimizer.zero_grad()\n","            loss.backward()\n","            for name, param in model.named_parameters():\n","                if name in params:\n","                    param.grad = params[name].grad\n","            inner_optimizer.step()\n","\n","        # Evaluate on test data\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for data, target in test_loader:\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                _, predicted = torch.max(output.data, 1)\n","                total += target.size(0)\n","                correct += (predicted == target).sum().item()\n","\n","        test_acc.append(correct / total)\n","\n","    avg_acc = sum(test_acc) / len(test_acc)\n","    print('Test Accuracy: {:.2f}%'.format(avg_acc * 100))\n","    return avg_acc\n"],"metadata":{"id":"WBxDKzzDmSZF","executionInfo":{"status":"ok","timestamp":1682822855547,"user_tz":240,"elapsed":148,"user":{"displayName":"Niki Monjazeb","userId":"10253055676513844231"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from numpy.random import RandomState\n","import numpy as np\n","\n","import torch.optim as optim\n","from torch.utils.data import Subset\n","from torchvision.datasets import MNIST, SVHN, STL10, Omniglot, Caltech101, Flowers102\n","from collections import OrderedDict\n","from torchvision import datasets, transforms\n","import torchvision.models as models\n","\n","normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n","\n","transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n","transform_train = transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor()])# , normalize\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","svhn_train = SVHN(root=\"./data\", split=\"train\", transform=transform_train, download=True)\n","svhn_loader = torch.utils.data.DataLoader(svhn_train, batch_size=128, shuffle=True)\n","\n","svhn_test = SVHN(root=\"./data\", split=\"test\", transform=transform_train, download=True)\n","svhn_test_loader = torch.utils.data.DataLoader(svhn_test, batch_size=128, shuffle=True)\n","\n","stl10_train = STL10(root=\"./data\", split=\"train\", transform=transform_train, download=True)\n","stl10_loader = torch.utils.data.DataLoader(stl10_train, batch_size=128, shuffle=True)\n","\n","stl10_test = STL10(root=\"./data\", split=\"test\", transform=transform_train, download=True)\n","stl10_test_loader = torch.utils.data.DataLoader(stl10_test, batch_size=128, shuffle=True)\n","\n","\n","##### Cifar Data\n","cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n","cifar_data_test = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n","    \n","#We need two copies of this due to weird dataset api \n","cifar_data_val = datasets.CIFAR10(root='.',train=True, transform=transform_val, download=True)\n","    \n","\n","accs = []\n","\n","for seed in range(1, 5):\n","  prng = RandomState(seed)\n","  random_permute = prng.permutation(np.arange(0, 1000))\n","  classes =  prng.permutation(np.arange(0,10))\n","  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n","  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n","\n","\n","  train_data = Subset(cifar_data, indx_train)\n","  val_data = Subset(cifar_data_val, indx_val)\n","\n","  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n","  \n","  train_loader = torch.utils.data.DataLoader(train_data,\n","                                             batch_size=128, \n","                                             shuffle=True)\n","\n","  val_loader = torch.utils.data.DataLoader(val_data,\n","                                           batch_size=128, \n","                                           shuffle=False)\n","\n","  train_tasks = [stl10_loader, svhn_loader]\n","  test_tasks = [stl10_test_loader, svhn_test_loader]\n","  val_tasks = [val_loader]\n","  # train_tasks = [train_loader]\n","  # test_tasks = [test_loader]\n","\n","\n","  model = MAMLLearner()\n","  model.to(device)\n","\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","\n","  for epoch in range(200): \n","    maml_train(model, device, train_tasks, test_tasks, optimizer, epoch)\n","\n","  accs.append(maml_test(model, device, val_tasks))\n","\n","accs = np.array(accs)\n","print('Acc over 5 instances: %.2f +- %.2f'%(accs.mean(),accs.std()))\n"," "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"id":"bEpGOBnNmTnV","executionInfo":{"status":"error","timestamp":1682822955741,"user_tz":240,"elapsed":75422,"user":{"displayName":"Niki Monjazeb","userId":"10253055676513844231"}},"outputId":"f444a1a9-d05c-4852-d45d-a3f6571f81bf"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ./data/train_32x32.mat\n","Using downloaded and verified file: ./data/test_32x32.mat\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Num Samples For Training 50 Num Samples For Val 400\n","Train Epoch: 0 \tLoss: 4.591238\n","Train Epoch: 1 \tLoss: 4.577613\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-4ad5162a4122>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmaml_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaml_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-21cd99b9d0c9>\u001b[0m in \u001b[0;36mmaml_train\u001b[0;34m(model, device, train_tasks, test_tasks, optimizer, epoch, num_inner_updates, display)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-5ed7130e2821>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["accs.append(maml_test(model, device, val_tasks))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwcfVjUkDNdz","executionInfo":{"status":"ok","timestamp":1682822861374,"user_tz":240,"elapsed":1272,"user":{"displayName":"Niki Monjazeb","userId":"10253055676513844231"}},"outputId":"946d60f8-866e-4c72-e7c5-27a67eecdbc5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 5.00%\n"]}]}]}